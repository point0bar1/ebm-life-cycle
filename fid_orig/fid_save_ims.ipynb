{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2553,"status":"ok","timestamp":1653255247114,"user":{"displayName":"Mitch Hill","userId":"17477852610183784654"},"user_tz":240},"id":"sykZ83w6yAPJ","outputId":"573bee7e-e416-4a28-bf0e-b1a211eee414"},"outputs":[],"source":["#####################\n","# ## COLAB SETUP ## #\n","#####################\n","\n","# set to True if running on colab to connect google drive, otherwise set to False\n","USE_COLAB = True\n","# set to True if using TPU connected to google cloud project\n","USE_TPU = True\n","# project name for TPU connection\n","PROJECT_NAME=\"\"\n","\n","if USE_COLAB:\n","  from google.colab import drive\n","  drive.mount('/content/drive')\n","\n","  if USE_TPU:\n","    # authenticate google cloud credentials\n","    from google.colab import auth\n","    auth.authenticate_user()\n","    # give colab access to project\n","    !gcloud config set project $PROJECT_NAME"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5603,"status":"ok","timestamp":1653255252711,"user":{"displayName":"Mitch Hill","userId":"17477852610183784654"},"user_tz":240},"id":"cm3_9YMVyHTA","outputId":"d8995f16-5d02-498a-a14d-cddcac9579bb"},"outputs":[],"source":["if USE_COLAB:\n","  !pip install -q tfds-nightly\n","  !pip install tensorflow_addons\n","  !git clone https://github.com/point0bar1/ebm-life-cycle"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1653255252711,"user":{"displayName":"Mitch Hill","userId":"17477852610183784654"},"user_tz":240},"id":"bjo9C_4axzSJ"},"outputs":[],"source":["####################\n","# ## PARAMETERS ## #\n","####################\n","\n","config = {\n","  # paths for connecting to cloud storage\n","  \"exp_folder\": 'fid_out/',  # folder name for results\n","  \"exp_dir\": '',  # location for saving files\n","\n","  # device type ('tpu' or 'gpu' or 'cpu')\n","  \"device_type\": 'tpu' if USE_TPU else 'gpu',\n","\n","  # exp params\n","  \"exp_type\": \"folder\",\n","  \"num_fid_rounds\": 520,\n","  \"batch_size\": 96,\n","  \"image_dims\": [128, 128, 3], # cifar10: 32x32, celeb_a 64x64, imagenet: 128x128\n","  \"split\": \"train\",\n","\n","  # data type and augmentation parameters\n","  \"data_type\": 'imagenet2012', # cifar10, celeb_a, imagenet2012\n","  \"random_crop\": False,\n","\n","  # ebm network\n","  \"net_type\": 'ebm_sngan',\n","  \"ebm_weights\": \"\",  # path to EBM weights\n","\n","  # langevin sampling parameters\n","  \"mcmc_steps\": 320,\n","  \"epsilon\": 3e-3,\n","  \"mcmc_init\": \"coop\",  # set to \"coop\" for generator or \"data\" for longrun from data\n","  \"mcmc_temp\": 1e-7,\n","  # clipping parameters\n","  \"clip_langevin_grad\": False,\n","  \"max_langevin_norm\": 0.25,\n","\n","  \"gen_type\": \"gen_sngan\",\n","  \"z_sz\": 128,\n","  \"gen_weights\": \"\"  # path to generator net weights\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":235},"executionInfo":{"elapsed":3525,"status":"error","timestamp":1653255256434,"user":{"displayName":"Mitch Hill","userId":"17477852610183784654"},"user_tz":240},"id":"Z6K4-XW-xgXz","outputId":"18abc1cb-27e0-4aa5-e530-21fae51222b8"},"outputs":[],"source":["# save images from tf2 model to png files to use original fid code for evaluation\n","\n","import os\n","import sys\n","from datetime import datetime\n","import pickle\n","from tqdm import tqdm\n","import importlib\n","from pathlib import Path\n","\n","import numpy as np\n","from PIL import Image\n","\n","import tensorflow as tf\n","import tensorflow_datasets as tfds\n","\n","sys.path.insert(0, '/content/ebm-life-cycle')\n","from init import init_strategy, initialize_nets_and_optim, initialize_data\n","from data import get_dataset\n","from utils import setup_exp, plot_ims\n","\n","import argparse\n","\n","\n","def save_samples(strategy, config, ebm, gen=None, train_iterator=None, save_str='samples.pdf'):\n","\n","  @tf.function\n","  def langevin_update(states_in):\n","    if config['mcmc_init'] == 'coop':\n","      # re-draw samples to avoid duplication on tpu device\n","      images_samp = tf.identity(gen(states_in))\n","    else:\n","      images_samp = tf.identity(states_in)\n","\n","    # initial samples for visual check\n","    images_samp_init = tf.identity(images_samp)\n","\n","    # langevin updates\n","    if config['mcmc_steps'] > 0:\n","      for i in tf.range(int(config['mcmc_steps'])):\n","        with tf.GradientTape() as tape:\n","          tape.watch(images_samp)\n","          energy = tf.math.reduce_sum(ebm(images_samp, training=False)) / config['mcmc_temp']\n","        grads = tape.gradient(energy, images_samp)\n","        # clip gradient norm (set to large value that won't interfere with standard dynamics)\n","        if config['clip_langevin_grad']:\n","          grads = tf.clip_by_norm(grads, config['max_langevin_norm'] / ((config['epsilon'] ** 2) / 2), axes=[1, 2, 3])\n","\n","        # update images\n","        images_samp -= ((config['epsilon'] ** 2) / 2) * grads\n","        images_samp += config['epsilon'] * tf.random.normal(shape=tpu_tensor_size)\n","\n","    return images_samp, images_samp_init\n","\n","  per_replica_batch_size = config['batch_size'] // strategy.num_replicas_in_sync\n","  images_np_1 = np.zeros([0] + config['image_dims'])\n","  images_np_2 = np.zeros([0] + config['image_dims'])\n","\n","  for i in range(config['num_fid_rounds']):\n","    print('Batch {} of {}'.format(i+1, config['num_fid_rounds']))\n","\n","    # data images\n","    images_data = next(train_iterator)\n","\n","    # generate samples from model\n","    if config['mcmc_init'] == 'data':\n","      sample_init = next(gen)\n","    elif config['mcmc_init'] == 'coop':\n","      z_init_tf = gen.generate_latent_z(config['batch_size'])\n","      def get_z_init(ctx):\n","        rep_id = ctx.replica_id_in_sync_group\n","        return z_init_tf[(rep_id*per_replica_batch_size):((rep_id+1)*per_replica_batch_size)]\n","      sample_init = strategy.experimental_distribute_values_from_function(get_z_init)\n","    else:\n","      raise ValueError('Invalid mcmc_init')\n","\n","    # run langevin updates on initial state\n","    images_sample, images_sample_init = strategy.run(langevin_update, args=(sample_init,))\n","\n","    # visualize initial and final samples for first batch\n","    if i == 0:\n","      plot_ims(os.path.join(config['exp_dir'], config['exp_folder'], 'images/' + save_str), \n","               strategy.gather(images_sample, 0))\n","      plot_ims(os.path.join(config['exp_dir'], config['exp_folder'], 'images/init_' + save_str), \n","               strategy.gather(images_sample_init, 0))\n","      plot_ims(os.path.join(config['exp_dir'], config['exp_folder'], 'images/data_' + save_str), \n","               strategy.gather(images_data, 0))\n","\n","    # record batch images\n","    p1 = Path(os.path.join(config['exp_dir'], config['exp_folder'], 'numpy_out/images1.npy'))\n","    with p1.open('ab') as f:\n","        images_data_rescale = np.rint(255 * (np.clip(strategy.gather(images_data, 0).numpy(), -1, 1) + 1) / 2)\n","        np.save(f, images_data_rescale.astype(np.uint8))\n","    p2 = Path(os.path.join(config['exp_dir'], config['exp_folder'], 'numpy_out/images2.npy'))\n","    with p2.open('ab') as f:\n","        images_sample_rescale = np.rint(255 * (np.clip(strategy.gather(images_sample, 0).numpy(), -1, 1) + 1) / 2)\n","        np.save(f, images_sample_rescale.astype(np.uint8))\n","\n","\n","###############\n","# ## SETUP ## #\n","###############\n","\n","# setup folders, save code, set seed and get device\n","setup_exp(os.path.join(config['exp_dir'], config['exp_folder']), \n","          ['images', 'numpy_out'],\n","          [os.path.join(config['exp_dir'], code_file) for \n","           code_file in ['fid_save_ims.ipynb', 'nets.py', 'utils.py', 'data.py', 'init.py']],\n","          gs_path=None, save_to_cloud=False)\n","\n","if config['device_type'] == 'tpu':\n","  resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n","  tf.config.experimental_connect_to_cluster(resolver)\n","  # This is the TPU initialization code that has to be at the beginning.\n","  tf.tpu.experimental.initialize_tpu_system(resolver)\n","  print(\"All devices: \", tf.config.list_logical_devices('TPU'))\n","  # Set up TPU Distribution\n","  strategy = tf.distribute.TPUStrategy(resolver)\n","else:\n","  strategy = init_strategy(config)\n","\n","\n","##################################################\n","# ## INITIALIZE NETS, DATA, PERSISTENT STATES ## #\n","##################################################\n","\n","# load nets and optim\n","ebm, _, gen, _ = initialize_nets_and_optim(config, strategy)\n","ebm.trainable = False\n","if gen is not None:\n","  gen.trainable = False\n","\n","# test deterministic output of ebm\n","with strategy.scope():\n","  state_test = tf.random.normal(shape=[3]+config['image_dims'])\n","  ebm_out_1 = ebm(state_test)\n","  ebm_out_2 = ebm(state_test[0:2])\n","ebm_out_1 = strategy.gather(ebm_out_1, axis=0)\n","ebm_out_2 = strategy.gather(ebm_out_2, axis=0)\n","print('EBM Determinism Test (should be close to 0): ', \n","      tf.math.reduce_max(tf.math.abs(ebm_out_1[0] - ebm_out_2[0])))\n","\n","# test deterministic output of gen\n","if gen is not None:\n","  with strategy.scope():\n","    gen_z = gen.generate_latent_z(3)\n","    gen_out_1 = gen(gen_z)\n","    gen_out_2 = gen(gen_z[0:2])\n","  gen_out_1 = strategy.gather(gen_out_1, axis=0)\n","  gen_out_2 = strategy.gather(gen_out_2, axis=0)\n","  print('Gen Determinism Test (should be close to 0): ', \n","        tf.math.reduce_max(tf.math.abs(gen_out_1[0] - gen_out_2[0])))\n","\n","# generator for data\n","train_iterator, _, _ = initialize_data(config, strategy)\n","if config['mcmc_init'] == 'data':\n","  # generator for data mcmc init\n","  gen, _, _ = initialize_data(config, strategy)\n","\n","# Calculate per replica batch size, and distribute the datasets\n","per_replica_batch_size = config['batch_size'] // strategy.num_replicas_in_sync\n","batch_size = per_replica_batch_size * strategy.num_replicas_in_sync\n","tpu_tensor_size = [per_replica_batch_size] + config['image_dims']\n","\n","# save bank of data and model samples as two np arrays\n","save_samples(strategy, config, ebm, gen, train_iterator)"]}],"metadata":{"accelerator":"TPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"fid_save_ims.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
